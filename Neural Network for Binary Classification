# **Neural Network for Binary Classification**
This project implements a **feedforward neural network** using TensorFlow and Keras to classify a given dataset into two categories. The dataset is preprocessed, balanced using **SMOTE (Synthetic Minority Over-sampling Technique)**, and then used to train a deep learning model.

---

## **üìå Project Overview**
### ‚úÖ What was done?
- **Data Preprocessing**: Standardized the dataset using **StandardScaler** for optimal performance.
- **Handling Class Imbalance**: Applied **SMOTE** to oversample the minority class and balance the dataset.
- **Neural Network Model**: Designed a **3-layer feedforward neural network** with **ReLU activation** in hidden layers and **Sigmoid activation** in the output layer.
- **Compilation & Training**: Used **binary cross-entropy loss** and **Adam optimizer** to train the model for **100 epochs**.
- **Prediction & Evaluation**: Generated predictions and evaluated performance using **accuracy score and classification report**.

### üéØ Why was it done?
- **Improve Model Accuracy**: Standardization ensures that all features contribute equally.
- **Handle Imbalanced Data**: SMOTE prevents the model from being biased toward the majority class.
- **Efficient Training**: A simple yet effective architecture was chosen to balance **training speed and accuracy**.

---

## **üìä Understanding Key Concepts**
### **1Ô∏è‚É£ Neural Network Layers**
- **Input Layer**: Accepts the input features.
- **Hidden Layers**: Two fully connected (dense) layers with **32 neurons each**, using **ReLU activation** for non-linearity.
- **Output Layer**: A **single neuron** with **Sigmoid activation** to classify between 0 and 1.

### **2Ô∏è‚É£ Activation Functions**
- **ReLU (Rectified Linear Unit)**: Introduces non-linearity, helping the model learn complex patterns.
- **Sigmoid**: Maps output to a probability range (0 to 1) for binary classification.

### **3Ô∏è‚É£ Optimizer - Adam**
- **Adam (Adaptive Moment Estimation)** is used because it adapts learning rates and speeds up convergence.

### **4Ô∏è‚É£ Loss Function - Binary Cross-Entropy**
- Since it's a **binary classification problem**, `binary_crossentropy` is used to measure prediction error.

### **5Ô∏è‚É£ Epochs & Batch Size**
- **Epochs**: The number of times the model sees the **entire dataset**.  
  - We used **100 epochs** to ensure the model converges.
- **Batch Size**: Number of samples processed before updating weights.  
  - We used **32**, a common choice for stable training.

### **6Ô∏è‚É£ SMOTE - Handling Imbalanced Data**
- **Why?** If one class has significantly fewer samples, the model may predict only the majority class.
- **How?** SMOTE generates synthetic samples of the minority class, balancing the dataset.

---
---

## **üìå Results & Evaluation**
After training, the model is evaluated on the test dataset.  
```bash
Neural Network Model Accuracy: 0.92  # Example output
```
A **classification report** is also generated, showing **precision, recall, and F1-score**.

---

## **üìú Conclusion**
This neural network efficiently classifies the dataset, overcoming class imbalance with **SMOTE** and using **ReLU-based dense layers** for optimal learning. The model achieves **high accuracy** and can be fine-tuned with hyperparameter adjustments.

---
